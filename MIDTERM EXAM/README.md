# 🕷️ 多執行緒網頁爬蟲

## 📌 原理與使用說明（最明顯的區塊）

本專案使用 Python 從零實作一個**多執行緒網頁爬蟲**。目的是展示對系統層級程式設計的理解，透過執行緒、多層遞迴、URL 分析與同一網域過濾等概念進行實作。

所有程式碼皆為原創，僅參考了像是 `requests`、`threading` 與 `BeautifulSoup` 等函式庫的官方文件。

### ✅ 來源與引用說明

| 檔案                 | 來源    | 說明                                                         |
| ------------------ | ----- | ---------------------------------------------------------- |
| `crawler.py`       | 原創 🧠 | 由本人撰寫，部分區段由 ChatGPT 協助補強。參考官方文件（如 requests/threading/bs4）。 |
| `utils.py`         | 原創 🧠 | 自訂函式，用來檢查 URL 有效性與正規化。                                     |
| `requirements.txt` | 原創 🧠 | 根據專案實際使用的函式庫整理出來的相依列表。                                     |
| `output.txt`       | 執行產物  | 執行爬蟲後自動產生，儲存所有擷取到的網址。                                      |

🔧 **修改說明與註解**：

* 使用 `books.toscrape.com` 作為爬蟲目標網域。
* 在 `crawler.py` 的 `crawl()` 函式中加入 `print()` 除錯訊息與即時回饋。
* 程式內含註解，說明爬蟲邏輯（如：網域過濾、多執行緒邏輯、例外處理等）。

---

## 功能特色

* 使用 Python `threading` 模組進行多執行緒爬取
* 支援遞迴式的深度限制爬蟲
* 過濾並僅爬取同網域的內部連結
* 將結果簡單儲存於 `output.txt`

---

## 使用方式

### 1. 安裝所需套件：

```bash
pip install -r requirements.txt
```

### 2. 執行爬蟲：

```bash
python crawler.py
```

* 從 `http://books.toscrape.com` 開始爬取
* 擷取到的連結會儲存至 `output.txt`
* 只會追蹤同一網域內的內部連結
* 執行過程中會在終端顯示即時進度與結果

---

## 備註

* 可以在 `crawler.py` 中修改爬取網域與最大深度參數
* 每次執行會覆蓋掉原有的 `output.txt`
* 包含例外處理機制，可略過無法連線或錯誤的網頁

---